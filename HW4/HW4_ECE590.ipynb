{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 590, Fall 2019 \n",
    "## Problem Set 4\n",
    "* ### __Important :__  You are not allowed to use built-in optimizers from Pytorch or any other Python Deep Learning environment. \n",
    "\n",
    "## Full name: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (First-order Optimization Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe data features can be extracted as train_dataset/test_dataset.data\\nThe labels can be extracted as train_dataset/test_dataset.targets \\nNote that the data are 28x28 images which you will have to transform to vectors.  \\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Download and prepare the MNIST data set \n",
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "\"\"\"\n",
    "The data features can be extracted as train_dataset/test_dataset.data\n",
    "The labels can be extracted as train_dataset/test_dataset.targets \n",
    "Note that the data are 28x28 images which you will have to transform to vectors.  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent\n",
      "Epoch: 1, Loss: 410.0344, Accuracy: 74.2000%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 2, Loss: 980.5943, Accuracy: 72.4000%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 3, Loss: 1297.1460, Accuracy: 71.8200%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 4, Loss: 1321.8522, Accuracy: 72.4300%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 5, Loss: 1792.0869, Accuracy: 71.4500%, Batch Size: 1, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 435.6182, Accuracy: 41.8700%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 2, Loss: 634.6593, Accuracy: 50.0300%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 3, Loss: 1103.9699, Accuracy: 49.3900%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 4, Loss: 1664.8859, Accuracy: 40.4800%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 5, Loss: 2320.2347, Accuracy: 33.9900%, Batch Size: 1, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 0.5395, Accuracy: 83.8600%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 2, Loss: 0.4677, Accuracy: 87.2400%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 3, Loss: 0.4959, Accuracy: 88.4100%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 4, Loss: 0.4950, Accuracy: 89.1000%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 5, Loss: 0.4648, Accuracy: 89.7600%, Batch Size: 500, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 0.9451, Accuracy: 82.0100%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 2, Loss: 1.7780, Accuracy: 69.1600%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 3, Loss: 1.0315, Accuracy: 79.9800%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 4, Loss: 1.4169, Accuracy: 83.6900%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 5, Loss: 1.5673, Accuracy: 55.3700%, Batch Size: 500, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.9933, Accuracy: 9.8800%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 2, Loss: 13.9794, Accuracy: 10.0700%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 3, Loss: 13.9587, Accuracy: 10.4900%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 4, Loss: 13.9312, Accuracy: 11.0300%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 5, Loss: 13.8968, Accuracy: 12.0000%, Batch Size: 60000, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.9526, Accuracy: 7.4900%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 2, Loss: 13.9384, Accuracy: 7.7000%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 3, Loss: 13.9174, Accuracy: 8.1900%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 4, Loss: 13.8894, Accuracy: 8.9900%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 5, Loss: 13.8546, Accuracy: 9.8400%, Batch Size: 60000, Lambda:1.00\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "## SGD correction\n",
    "class logistic_regression_sgd(nn.Module):\n",
    "    def __init__(self, n_features, out_features, bias, alpha, lambdaf):\n",
    "        super(logistic_regression_sgd, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_features, n_classes)\n",
    "        self.alpha = alpha\n",
    "        self.lambdaf = lambdaf\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.linear1(x)\n",
    "        return a1\n",
    "    \n",
    "    def update(self, grad):\n",
    "        with torch.no_grad():\n",
    "            self.linear1.weight -= self.alpha * grad\n",
    "\n",
    "batch_size_list = [1,500,6*10**4]\n",
    "lambdaf_list = [.1,1]\n",
    "print(\"Stochastic Gradient Descent\")\n",
    "            \n",
    "for batch_size in batch_size_list:\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    for lambdaf in lambdaf_list:\n",
    "        alpha = .001\n",
    "        n_features = 784\n",
    "        n_classes = 10\n",
    "        \n",
    "        model = logistic_regression_sgd(n_features, n_classes, True, alpha, lambdaf = lambdaf)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        losses = []\n",
    "        epoch = 5\n",
    "        for i in range(epoch):\n",
    "            for (x_train, y_train) in trainloader:\n",
    "                x_train = Variable(x_train.view(-1, 28 * 28))\n",
    "                y_train = Variable(y_train)\n",
    "                y_hat_train = model(x_train) # Forward pass\n",
    "                lf = torch.norm(model.linear1.weight, p = 'fro')\n",
    "                loss = criterion(y_hat_train, y_train) + model.lambdaf * lf\n",
    "                loss.backward()      # backward Pass\n",
    "                model.update(model.linear1.weight.grad)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                correct = 0\n",
    "                for x_test, y_test in testloader:\n",
    "                    x_test = Variable(x_test.view(-1, 28 * 28))\n",
    "                    y_test = Variable(y_test)\n",
    "                    y_hat_test = model(x_test)\n",
    "                    test_loss += criterion(y_hat_test, y_test).item()\n",
    "                    pred = y_hat_test.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss = (test_loss*batch_size)/len(testloader.dataset)\n",
    "            print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}%, Batch Size: {}, Lambda:{:.2f}'.format(\n",
    "            i+1, test_loss, 100. * correct / len(testloader.dataset), batch_size, lambdaf))\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nestrov Method\n",
      "Epoch: 1, Loss: 97.9568, Accuracy: 72.9800%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 2, Loss: 242.1293, Accuracy: 86.6200%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 3, Loss: 408.0849, Accuracy: 83.6100%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 4, Loss: 493.4856, Accuracy: 84.0400%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 5, Loss: 540.1819, Accuracy: 81.8600%, Batch Size: 1, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 117.1002, Accuracy: 54.6400%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 2, Loss: 256.6457, Accuracy: 44.1200%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 3, Loss: 396.3244, Accuracy: 38.1800%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 4, Loss: 666.5719, Accuracy: 38.5500%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 5, Loss: 776.7982, Accuracy: 39.7100%, Batch Size: 1, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 1.1624, Accuracy: 80.4100%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 2, Loss: 0.6316, Accuracy: 85.5200%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 3, Loss: 0.5062, Accuracy: 87.7700%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 4, Loss: 0.4692, Accuracy: 88.9700%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 5, Loss: 0.4634, Accuracy: 89.6700%, Batch Size: 500, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 1.5307, Accuracy: 79.9900%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 2, Loss: 2.0998, Accuracy: 70.1600%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 3, Loss: 2.1017, Accuracy: 70.7300%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 4, Loss: 2.1571, Accuracy: 65.7300%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 5, Loss: 2.1520, Accuracy: 69.0600%, Batch Size: 500, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.9911, Accuracy: 7.5000%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 2, Loss: 13.9822, Accuracy: 7.5900%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 3, Loss: 13.9715, Accuracy: 7.7000%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 4, Loss: 13.9589, Accuracy: 7.9000%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 5, Loss: 13.9445, Accuracy: 8.0700%, Batch Size: 60000, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.8760, Accuracy: 9.9600%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 2, Loss: 13.8668, Accuracy: 10.0300%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 3, Loss: 13.8558, Accuracy: 10.1400%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 4, Loss: 13.8429, Accuracy: 10.2400%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 5, Loss: 13.8281, Accuracy: 10.4400%, Batch Size: 60000, Lambda:1.00\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Nestrov Method \n",
    "class logistic_regression_nestrov(nn.Module):\n",
    "    def __init__(self, n_features, out_features, bias, alpha, beta, lambdaf):\n",
    "        super(logistic_regression_nestrov, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_features, n_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.alpha = alpha\n",
    "        self.lambdan = 0\n",
    "        self.lambdaf = lambdaf\n",
    "        self.t = torch.zeros(n_classes,n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        #y = self.softmax(a1)\n",
    "        return y\n",
    "\n",
    "    def update(self, grad):\n",
    "        with torch.no_grad():\n",
    "            self.lambdan = (1+np.sqrt(1+4*self.lambdan**2))/2\n",
    "            next_lambdan = (1+np.sqrt(1+4*self.lambdan**2))/2\n",
    "            self.gamma = (1-self.lambdan)/next_lambdan\n",
    "            next_t = self.linear1.weight - self.alpha * grad\n",
    "            self.linear1.weight = torch.nn.Parameter((1 - self.gamma) * next_t + self.gamma * self.t)\n",
    "            self.t = next_t\n",
    "            \n",
    "batch_size_list = [1,500,6*10**4]\n",
    "lambdaf_list = [.1,1]\n",
    "print(\"Nestrov Method\")\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "    for lambdaf in lambdaf_list:\n",
    "        alpha = .001\n",
    "        n_features = 784\n",
    "        n_classes = 10\n",
    "        beta = .95\n",
    "      \n",
    "        model = logistic_regression_nestrov(n_features, n_classes, True, alpha, beta, lambdaf)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        losses = []\n",
    "        epoch = 5\n",
    "        for i in range(epoch):\n",
    "            for (x_train, y_train) in trainloader:\n",
    "                x_train = Variable(x_train.view(-1, 28 * 28))\n",
    "                y_train = Variable(y_train)\n",
    "                y_hat_train = model(x_train) # Forward pass\n",
    "                lf = torch.norm(model.linear1.weight, p = 'fro')\n",
    "                loss = criterion(y_hat_train, y_train) + model.lambdaf * lf\n",
    "                loss.backward()      # backward Pass\n",
    "                model.update(model.linear1.weight.grad)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                correct = 0\n",
    "                for x_test, y_test in testloader:\n",
    "                    x_test = Variable(x_test.view(-1, 28 * 28))\n",
    "                    y_test = Variable(y_test)\n",
    "                    y_hat_test = model(x_test)\n",
    "                    test_loss += criterion(y_hat_test, y_test).item()\n",
    "                    pred = y_hat_test.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss = (test_loss*batch_size)/len(testloader.dataset)\n",
    "            print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}%, Batch Size: {}, Lambda:{:.2f}'.format(\n",
    "            i+1, test_loss, 100. * correct / len(testloader.dataset), batch_size, lambdaf))\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum Method\n",
      "Epoch: 1, Loss: 4867.0266, Accuracy: 72.8500%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 2, Loss: 11637.2734, Accuracy: 67.4400%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 3, Loss: 15406.2179, Accuracy: 73.1100%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 4, Loss: 20238.5997, Accuracy: 72.0200%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 5, Loss: 23434.6364, Accuracy: 72.0900%, Batch Size: 1, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 20052.5431, Accuracy: 34.6200%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 2, Loss: 26226.6057, Accuracy: 35.9900%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 3, Loss: 29897.8855, Accuracy: 38.7200%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 4, Loss: 25690.0669, Accuracy: 45.8900%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 5, Loss: 41919.3584, Accuracy: 38.6300%, Batch Size: 1, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 1.9578, Accuracy: 72.6500%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 2, Loss: 14.6546, Accuracy: 63.6700%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 3, Loss: 10.0934, Accuracy: 69.7300%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 4, Loss: 36.6363, Accuracy: 61.5500%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 5, Loss: 52.0977, Accuracy: 53.2500%, Batch Size: 500, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 11.9787, Accuracy: 12.4500%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 2, Loss: 69.0069, Accuracy: 25.4700%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 3, Loss: 149.1322, Accuracy: 21.8600%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 4, Loss: 168.7829, Accuracy: 40.2300%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 5, Loss: 254.5430, Accuracy: 13.5300%, Batch Size: 500, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.9048, Accuracy: 11.8100%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 2, Loss: 13.8858, Accuracy: 12.1900%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 3, Loss: 13.8492, Accuracy: 12.9800%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 4, Loss: 13.7903, Accuracy: 14.6200%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 5, Loss: 13.7053, Accuracy: 17.0000%, Batch Size: 60000, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.9413, Accuracy: 9.4400%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 2, Loss: 13.9203, Accuracy: 9.8200%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 3, Loss: 13.8798, Accuracy: 10.5800%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 4, Loss: 13.8149, Accuracy: 12.1100%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 5, Loss: 13.7218, Accuracy: 14.6000%, Batch Size: 60000, Lambda:1.00\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Momentum Method corrected\n",
    "class logistic_regression_momentum(nn.Module):\n",
    "    def __init__(self, n_features, out_features, bias, alpha, beta, lambdaf):\n",
    "        super(logistic_regression_momentum, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_features, n_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.m = 0\n",
    "        self.lambdaf = lambdaf\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.linear1(x)\n",
    "        return a1\n",
    "    \n",
    "    def update(self, grad):\n",
    "        with torch.no_grad():\n",
    "            self.m = self.beta * self.m + grad\n",
    "            self.linear1.weight -= self.alpha * self.m\n",
    "            \n",
    "batch_size_list = [1,500,6*10**4]\n",
    "lambdaf_list = [.1,1]\n",
    "print(\"Momentum Method\")\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "    for lambdaf in lambdaf_list:\n",
    "        alpha = .001\n",
    "        n_features = 784\n",
    "        n_classes = 10\n",
    "        beta = .9\n",
    "\n",
    "        model = logistic_regression_momentum(n_features, n_classes, True, alpha, beta, lambdaf)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        losses = []\n",
    "        epoch = 5\n",
    "\n",
    "        for i in range(epoch):\n",
    "            for (x_train, y_train) in trainloader:\n",
    "                x_train = Variable(x_train.view(-1, 28 * 28))\n",
    "                y_train = Variable(y_train)\n",
    "                y_hat_train = model(x_train) # Forward pass\n",
    "                lf = torch.norm(model.linear1.weight, p = 'fro')\n",
    "                loss = criterion(y_hat_train, y_train) + model.lambdaf * lf\n",
    "                loss.backward()      # backward Pass\n",
    "                model.update(model.linear1.weight.grad)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                correct = 0\n",
    "                for x_test, y_test in testloader:\n",
    "                    x_test = Variable(x_test.view(-1, 28 * 28))\n",
    "                    y_test = Variable(y_test)\n",
    "                    y_hat_test = model(x_test)\n",
    "                    test_loss += criterion(y_hat_test, y_test).item()\n",
    "                    pred = y_hat_test.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss = (test_loss*batch_size)/len(testloader.dataset)\n",
    "            print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}%, Batch Size: {}, Lambda:{:.2f}'.format(\n",
    "            i+1, test_loss, 100. * correct / len(testloader.dataset), batch_size, lambdaf))\n",
    "        print(\"--------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSProp\n",
      "Epoch: 1, Loss: 10.0091, Accuracy: 77.1200%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 2, Loss: 19.4195, Accuracy: 75.6500%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 3, Loss: 17.6035, Accuracy: 78.1300%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 4, Loss: 19.4083, Accuracy: 76.9600%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 5, Loss: 20.4109, Accuracy: 78.9500%, Batch Size: 1, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 1.2869, Accuracy: 58.5700%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 2, Loss: 1.1432, Accuracy: 64.7700%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 3, Loss: 1.4570, Accuracy: 54.4000%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 4, Loss: 1.3223, Accuracy: 58.7400%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 5, Loss: 1.3090, Accuracy: 60.0100%, Batch Size: 1, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 0.4182, Accuracy: 88.2000%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 2, Loss: 0.3611, Accuracy: 89.7500%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 3, Loss: 0.3808, Accuracy: 89.3000%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 4, Loss: 0.4305, Accuracy: 88.3200%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 5, Loss: 0.4464, Accuracy: 88.9500%, Batch Size: 500, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 1.7005, Accuracy: 75.3200%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 2, Loss: 1.7693, Accuracy: 74.6200%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 3, Loss: 1.8363, Accuracy: 70.0200%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 4, Loss: 1.8598, Accuracy: 70.8100%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 5, Loss: 1.8533, Accuracy: 72.7500%, Batch Size: 500, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 12.9865, Accuracy: 27.4600%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 2, Loss: 12.2000, Accuracy: 46.3700%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 3, Loss: 11.5323, Accuracy: 59.9300%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 4, Loss: 10.9484, Accuracy: 65.9600%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 5, Loss: 10.4299, Accuracy: 69.4600%, Batch Size: 60000, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.0315, Accuracy: 33.4900%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 2, Loss: 12.4237, Accuracy: 49.5500%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 3, Loss: 11.9454, Accuracy: 59.6900%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 4, Loss: 11.5641, Accuracy: 66.2000%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 5, Loss: 11.2579, Accuracy: 69.8700%, Batch Size: 60000, Lambda:1.00\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RMSProp Corrected\n",
    "class logistic_regression_rmsprop(nn.Module):\n",
    "    def __init__(self, n_features, out_features, bias, alpha, beta, gamma, epsilon, lambdaf):\n",
    "        super(logistic_regression_rmsprop, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_features, n_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.b = torch.zeros(n_classes,n_features)\n",
    "        self.gamma = gamma\n",
    "        self.v = torch.zeros(n_classes,n_features)\n",
    "        self.epsilon = epsilon\n",
    "        self.lambdaf = lambdaf\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a1 = self.linear1(x)\n",
    "        return a1\n",
    "    \n",
    "    def update(self, grad):\n",
    "        with torch.no_grad():\n",
    "            self.v = self.beta * self.v + (1-self.beta) * (grad * grad)\n",
    "            self.b = self.gamma * 1/ (torch.sqrt(self.v) + self.epsilon)\n",
    "            self.linear1.weight -= self.alpha * self.b * grad \n",
    "            \n",
    "batch_size_list = [1,500,6*10**4]\n",
    "lambdaf_list = [.1,1]\n",
    "print(\"RMSProp\")\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "    for lambdaf in lambdaf_list:\n",
    "        alpha = .001\n",
    "        n_features = 784\n",
    "        n_classes = 10\n",
    "        beta = .9\n",
    "        gamma = 1\n",
    "        epsilon = 10**-8\n",
    "\n",
    "        model = logistic_regression_rmsprop(n_features, n_classes, True, alpha, beta, gamma, epsilon, lambdaf)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        losses = []\n",
    "        epoch = 5\n",
    "        for i in range(epoch):\n",
    "            for (x_train, y_train) in trainloader:\n",
    "                x_train = Variable(x_train.view(-1, 28 * 28))\n",
    "                y_train = Variable(y_train)\n",
    "                y_hat_train = model(x_train) # Forward pass\n",
    "                lf = torch.norm(model.linear1.weight, p = 'fro')\n",
    "                loss = criterion(y_hat_train, y_train) + model.lambdaf * lf\n",
    "                loss.backward()      # backward Pass\n",
    "                model.update(model.linear1.weight.grad)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                correct = 0\n",
    "                for x_test, y_test in testloader:\n",
    "                    x_test = Variable(x_test.view(-1, 28 * 28))\n",
    "                    y_test = Variable(y_test)\n",
    "                    y_hat_test = model(x_test)\n",
    "                    test_loss += criterion(y_hat_test, y_test).item()\n",
    "                    pred = y_hat_test.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss = (test_loss*batch_size)/len(testloader.dataset)\n",
    "            print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}%, Batch Size: {}, Lambda:{:.2f}'.format(\n",
    "            i+1, test_loss, 100. * correct / len(testloader.dataset), batch_size, lambdaf))\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 6.7458, Accuracy: 74.4600%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 2, Loss: 6.1934, Accuracy: 75.2300%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 3, Loss: 5.2459, Accuracy: 75.6300%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 4, Loss: 6.7090, Accuracy: 75.7300%, Batch Size: 1, Lambda:0.10\n",
      "Epoch: 5, Loss: 5.1953, Accuracy: 76.7000%, Batch Size: 1, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 5.1661, Accuracy: 44.7800%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 2, Loss: 2.8529, Accuracy: 57.9300%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 3, Loss: 7.0833, Accuracy: 45.1400%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 4, Loss: 4.8426, Accuracy: 46.0500%, Batch Size: 1, Lambda:1.00\n",
      "Epoch: 5, Loss: 4.7576, Accuracy: 44.2300%, Batch Size: 1, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 0.5876, Accuracy: 81.7900%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 2, Loss: 1.0104, Accuracy: 76.6000%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 3, Loss: 1.8205, Accuracy: 66.6400%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 4, Loss: 2.5209, Accuracy: 64.5200%, Batch Size: 500, Lambda:0.10\n",
      "Epoch: 5, Loss: 2.6009, Accuracy: 67.3300%, Batch Size: 500, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 1.4053, Accuracy: 55.5700%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 2, Loss: 3.6026, Accuracy: 23.9100%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 3, Loss: 7.9710, Accuracy: 30.0500%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 4, Loss: 6.5372, Accuracy: 30.9500%, Batch Size: 500, Lambda:1.00\n",
      "Epoch: 5, Loss: 6.9768, Accuracy: 30.6100%, Batch Size: 500, Lambda:1.00\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.3787, Accuracy: 21.7300%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 2, Loss: 13.0866, Accuracy: 26.9200%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 3, Loss: 12.8052, Accuracy: 33.9400%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 4, Loss: 12.5312, Accuracy: 41.2200%, Batch Size: 60000, Lambda:0.10\n",
      "Epoch: 5, Loss: 12.2630, Accuracy: 48.8800%, Batch Size: 60000, Lambda:0.10\n",
      "--------------------------------\n",
      "Epoch: 1, Loss: 13.5064, Accuracy: 18.1700%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 2, Loss: 13.2797, Accuracy: 24.4600%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 3, Loss: 13.0599, Accuracy: 31.7100%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 4, Loss: 12.8454, Accuracy: 39.1400%, Batch Size: 60000, Lambda:1.00\n",
      "Epoch: 5, Loss: 12.6350, Accuracy: 45.3800%, Batch Size: 60000, Lambda:1.00\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "#  Adam Corrected\n",
    "class logistic_regression_adam(nn.Module):\n",
    "    def __init__(self, n_features, out_features, bias, alpha, beta1, beta2, epsilon, lambdaf):\n",
    "        super(logistic_regression_adam, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_features, n_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m1 = torch.zeros(n_classes,n_features)\n",
    "        self.m2 = torch.zeros(n_classes,n_features)\n",
    "        self.k = 0\n",
    "        self.epsilon = epsilon \n",
    "        self.lambdaf = lambdaf\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.linear1(x)\n",
    "        return a1\n",
    "    \n",
    "    def update(self, grad):\n",
    "        with torch.no_grad():\n",
    "            self.k += 1\n",
    "            self.m1 = (self.beta1) * self.m1 + (1-self.beta1) * grad\n",
    "            m1_corrected = self.m1 / (1 - (self.beta1 ** self.k))\n",
    "            self.m2 = (self.beta2) * self.m2 + (1-self.beta2) * grad * grad\n",
    "            m2_corrected = self.m2 / (1 - (self.beta2 ** self.k))  \n",
    "            self.linear1.weight -= self.alpha * m1_corrected /(self.epsilon + torch.sqrt(m2_corrected))\n",
    "\n",
    "    \n",
    "for batch_size in batch_size_list:\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    for lambdaf in lambdaf_list:        \n",
    "        alpha = .001\n",
    "        n_features = 784\n",
    "        n_classes = 10\n",
    "        beta1 = .9\n",
    "        beta2 = .999\n",
    "        epsilon = 10**-8\n",
    "\n",
    "        model = logistic_regression_adam(n_features, n_classes, True, alpha, beta1, beta2, epsilon, lambdaf)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        losses = []\n",
    "        epoch = 5\n",
    "\n",
    "        for i in range(epoch):\n",
    "            for (x_train, y_train) in trainloader:\n",
    "                x_train = Variable(x_train.view(-1, 28 * 28))\n",
    "                y_train = Variable(y_train)\n",
    "                y_hat_train = model(x_train) # Forward pass\n",
    "                lf = torch.norm(model.linear1.weight, p = 'fro')\n",
    "                loss = criterion(y_hat_train, y_train) + model.lambdaf * lf\n",
    "                loss.backward()      # backward Pass\n",
    "                model.update(model.linear1.weight.grad)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                correct = 0\n",
    "                for x_test, y_test in testloader:\n",
    "                    x_test = Variable(x_test.view(-1, 28 * 28))\n",
    "                    y_test = Variable(y_test)\n",
    "                    y_hat_test = model(x_test)\n",
    "                    test_loss += criterion(y_hat_test, y_test).item()\n",
    "                    pred = y_hat_test.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                correct = 0\n",
    "                for x_test, y_test in testloader:\n",
    "                    x_test = Variable(x_test.view(-1, 28 * 28))\n",
    "                    y_test = Variable(y_test)\n",
    "                    y_hat_test = model(x_test)\n",
    "                    test_loss += criterion(y_hat_test, y_test).item()\n",
    "                    pred = y_hat_test.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "\n",
    "            test_loss = (test_loss*batch_size)/len(testloader.dataset)\n",
    "            print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}%, Batch Size: {}, Lambda:{:.2f}'.format(\n",
    "            i+1, test_loss, 100. * correct / len(testloader.dataset), batch_size, lambdaf))\n",
    "        print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (Newton’s Method for Non-linear Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. The following vector minimizes the function f(x) using Newton's Method: [-3.46573590e-01  6.97888745e-12]. The number of iterations was 14. The value of the function is 2.5592666966582156.\n",
      "b. The following vector minimizes the function f(x) using Newton's Method and approximated Hessian: [-3.46573590e-01  2.07985301e-18]. The number of iterations was 12. The value of the function is 2.5592666966582156.\n"
     ]
    }
   ],
   "source": [
    "# Put your code here \n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    \"\"\" Function \"\"\"\n",
    "    return np.exp(x[0]+3*x[1]-.1)+np.exp(x[0]-3*x[1]-.1)+np.exp(-x[0]-.1)\n",
    "\n",
    "def grad(x):\n",
    "    \"\"\" Gradient of function f(x)\"\"\"\n",
    "    dx1 = np.exp(x[0]+3*x[1]-.1)+np.exp(x[0]-3*x[1]-.1)-np.exp(-x[0]-.1)\n",
    "    dx2 = np.exp(x[0]+3*x[1]-.1)*3+np.exp(x[0]-3*x[1]-.1)*(-3)\n",
    "    return np.array([dx1,dx2])\n",
    "\n",
    "def H(x, approx = False):\n",
    "    \"\"\" \n",
    "    Hessian of function f(x)\n",
    "    \"\"\"\n",
    "    dx1_2 = np.exp(x[0]+3*x[1]-.1)+np.exp(x[0]-3*x[1]-.1)+np.exp(-x[0]-.1)\n",
    "    dx2_2 = np.exp(x[0]+3*x[1]-.1)*9+np.exp(x[0]-3*x[1]-.1)*9\n",
    "    if not approx:\n",
    "        dx1x2 = np.exp(x[0]+3*x[1]-.1)*3+np.exp(x[0]-3*x[1]-.1)*-3\n",
    "        H = np.array([[dx1_2,dx1x2],[dx1x2,dx2_2]])\n",
    "    else:\n",
    "        H = np.diag([dx1_2,dx2_2])\n",
    "    return H\n",
    "                                          \n",
    "                                          \n",
    "def newton(x,f,grad,H,alpha = .1, approx = False, tol = .001):\n",
    "    \"\"\" Newthon's method optmization using Hessian and gradient\"\"\"\n",
    "    diff = np.inf\n",
    "    count = 0\n",
    "    while diff > tol:\n",
    "        count += 1\n",
    "        x_next = x - alpha * np.dot(np.linalg.inv(H(x, approx)), grad(x))\n",
    "        diff = np.linalg.norm(x_next - x)\n",
    "        x = x_next\n",
    "    return x, count\n",
    "\n",
    "\n",
    "## \n",
    "alpha = 1\n",
    "x = np.array([1.,3.])\n",
    "y,ny = newton(x,f,grad,H,alpha)\n",
    "z,nz = newton(x,f,grad,H,alpha, approx = True)\n",
    "\n",
    "print(\"a. The following vector minimizes the function f(x) using Newton's Method: {0}. The number of iterations was {1}. The value of the function is {2}.\".format(y,ny, f(y)))\n",
    "\n",
    "print(\"b. The following vector minimizes the function f(x) using Newton's Method and approximated Hessian: {0}. The number of iterations was {1}. The value of the function is {2}.\".format(z, nz, f(z)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Logistic Regression using Newton’s Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\" data preparation \"\"\"\n",
    "\n",
    "# read csv file \n",
    "df = pd.read_csv(\"breast_cancer.csv\") # df denotes a data frame \n",
    "\n",
    "# extract the 'diagnosis' column as your targets \n",
    "targets = df['diagnosis'].values\n",
    "\n",
    "# convert the entries of targets to 0/1 \n",
    "targets = (targets == \"M\") * 1\n",
    "     \n",
    "# extract your features data\n",
    "data = df.drop(['diagnosis', 'id'], axis=1).values\n",
    "\n",
    "# train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()\n",
    "X_test, y_test = torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 12\n",
    "class binary_classification(nn.Module):\n",
    "    def __init__(self, n_features, out_features, bias, step_size = .0001, approx = False):\n",
    "        super(binary_classification, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=n_features, out_features=1, bias= False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.step_size = step_size\n",
    "        self.approx = approx\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.linear(x)\n",
    "        y = self.sigmoid(a1)\n",
    "        return y\n",
    "    \n",
    "    def update(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            z = torch.mm(x,self.linear.weight.data.t())\n",
    "            d = torch.sigmoid(z) * (1-torch.sigmoid(z))\n",
    "            d = d.view(-1)\n",
    "            D = torch.diag(d)\n",
    "            H = torch.mm(torch.mm(x.t(),D),x)\n",
    "            if self.approx:\n",
    "                I = torch.eye(n_features, n_features)\n",
    "                H = torch.where(I == 0,I,H)\n",
    "                \n",
    "            aux = torch.sigmoid(z).view(-1)\n",
    "            grad = torch.mm(x.t(),(aux-y).reshape(-1,1))\n",
    "            self.linear.weight -= torch.nn.Parameter( (self.step_size * torch.mm(torch.inverse(H),grad)).t() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = binary_classification(n_features, 1, False)\n",
    "criterion = nn.BCELoss()\n",
    "losses = []\n",
    "num_iter = 700\n",
    "\n",
    "for i in range(num_iter):\n",
    "    y_hat_train = model(X_train)     # Forward Pass\n",
    "    loss = criterion(y_hat_train.squeeze(), y_train)\n",
    "    loss.backward()      # backward Pass\n",
    "    model.update(X_train, y_train)\n",
    "    losses.append(loss.item())\n",
    "        \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_hat_test = model(X_test)\n",
    "    loss = criterion(y_hat_test.squeeze(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure()\n",
    "plt.title(\"Newton method\")\n",
    "plt.plot(list(range(len(losses))), losses, color='blue', linewidth=2, label='Learning curve')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = binary_classification(n_features, 1, False, approx = True)\n",
    "criterion = nn.BCELoss()\n",
    "losses_approx = []\n",
    "num_iter = 700\n",
    "\n",
    "for i in range(num_iter):\n",
    "    y_hat_train = model(X_train)     # Forward Pass\n",
    "    loss = criterion(y_hat_train.squeeze(), y_train)\n",
    "    loss.backward()      # backward Pass\n",
    "    model.update(X_train, y_train)\n",
    "    losses_approx.append(loss.item())\n",
    "        \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_hat_test = model(X_test)\n",
    "    loss = criterion(y_hat_test.squeeze(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.figure()\n",
    "plt.title(\"Newton method using Hessian approximation\")\n",
    "plt.plot(list(range(len(losses_approx))), losses_approx, color='blue', linewidth=2, label='Learning curve')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
